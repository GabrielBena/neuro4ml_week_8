{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuro4ML Week 8: Neuromorphic Computing\n",
    "\n",
    "## Introduction \n",
    "\n",
    "Neuromorphic engineering is a field that aims to design and build artificial neural systems that mimic the architecture and principles of biological neural networks. Unlike traditional von Neumann computing architectures, neuromorphic chips:\n",
    "\n",
    "1. Process information in a parallel, event-driven manner\n",
    "2. Integrate memory and computation\n",
    "3. Operate with extremely low power consumption\n",
    "\n",
    "### Why Trade-off Power and Accuracy?\n",
    "\n",
    "Traditional deep learning models running on GPUs or CPUs consume significant power (often hundreds of watts). In contrast, the human brain processes complex information while consuming only ~20 watts. Neuromorphic chips aim to bridge this efficiency gap by:\n",
    "\n",
    "- Using spike-based computation\n",
    "- Implementing local learning rules\n",
    "- Exploiting sparse, event-driven processing\n",
    "\n",
    "However, these benefits often come with reduced accuracy compared to traditional deep learning approaches. Understanding and optimizing this trade-off is crucial for deploying neural networks in power-constrained environments like mobile devices or IoT sensors.\n",
    "\n",
    "## Exercise Overview\n",
    "\n",
    "In this exercise, you will:\n",
    "1. Implement a simple neuromorphic chip simulator\n",
    "2. Train SNNs with different architectures\n",
    "3. Analyze the power-accuracy trade-off\n",
    "4. Explore how different parameters affect this trade-off\n",
    "\n",
    "This will also serve as a solid introduction on how to effectively train SNNs using modern packages such as SNNTorch ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install snntorch\n",
    "    !https://github.com/GabrielBena/neuro4ml_week_8.git\n",
    "    import sys\n",
    "    sys.path.append('neuro4ml_week_8')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from chip import NeuromorphicChip\n",
    "from models import SNNModel\n",
    "\n",
    "# For automatic reloading of external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Mapping\n",
    "\n",
    "To complete this first question you need to implement the functions necessary to map your network on the chip.\n",
    "\n",
    "- Go to chip.py and implement the `calculate_memory_usage` and `map` methods\n",
    "- Go to models.py and implement the `n_neurons` and `n_synapses` properties\n",
    "- Run the following cell to check your implementation\n",
    "\n",
    "This is what you should see:\n",
    "\n",
    "![Expected simulation results](sim_results.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip = NeuromorphicChip()\n",
    "\n",
    "dims = (128, 100, 10)\n",
    "n_timesteps = 100\n",
    "seed = 42\n",
    "snn = SNNModel(n_in=dims[0], n_hidden=dims[1], n_out=dims[-1], beta=0.95, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random input (seed is fixed to 42 for reproducibility)\n",
    "torch.manual_seed(seed)\n",
    "input_data = torch.randn(1, n_timesteps, dims[0]) * 10  # 100 timesteps\n",
    "\n",
    "# Map the network on the chip\n",
    "chip.map(snn)\n",
    "# Run the network\n",
    "output, results = chip.run(input_data=input_data)\n",
    "\n",
    "print(\"\\nSimulation Results:\")\n",
    "print(f\"Energy consumption: {results['total_energy_nJ']/1000:.2f} ÂµJ\")\n",
    "print(f\"Memory usage: {results['memory_usage_bytes']/1024:.2f} KB\")\n",
    "print(f\"Total neuron updates: {results['neuron_updates']}\")\n",
    "print(f\"Total synapse events: {results['synapse_events']}\")\n",
    "print(f\"Average spike rate: {results['spike_rate']:.3f}\")\n",
    "print(f\"Total spikes: {results['total_spikes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Proper Training\n",
    "\n",
    "In this exercise you will train a SNN on the Randman dataset.\n",
    "\n",
    "- Go to training.py and complete the `SNNTrainer` class, in particular the `calculate_accuracy` method\n",
    "- Run the following cell to train your network\n",
    "- Take a look at the training and testing metrics, especially the accuracy and energy consumption\n",
    "- Start experimenting with different architectures and parameters to see how they affect the accuracy and energy consumption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import get_dataloaders, SNNTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader, test_loader, dataset = get_dataloaders(\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = next(iter(train_loader))\n",
    "print(data.shape, labels.shape)  # batch_size x timesteps x n_in. 1st and 2nd dims are swapped when passed to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn_config = {\n",
    "    \"n_hidden\": 128,\n",
    "    \"beta\": 0.95,\n",
    "    \"seed\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "snn = SNNModel(\n",
    "    n_hidden=snn_config[\"n_hidden\"],\n",
    "    beta=snn_config[\"beta\"],\n",
    "    seed=snn_config[\"seed\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = SNNTrainer(snn, learning_rate=1e-3, lr_gamma=0.9, config=snn_config)\n",
    "# Train the model\n",
    "trainer.train(train_loader, test_loader, n_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results\n",
    "- We can plot the accuracy and energy consumption as a function of the epoch\n",
    "- We see that the accuracy is improving but the energy consumption is also increasing\n",
    "- This is a trade-off that we need to be aware of when training SNNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.pd_results\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(\n",
    "    data=results, x=\"epoch\", y=\"accuracy\", ax=ax, label=\"Accuracy\", legend=False\n",
    ")\n",
    "ax2 = ax.twinx()\n",
    "sns.lineplot(\n",
    "    data=results,\n",
    "    x=\"epoch\",\n",
    "    y=\"total_energy_nJ\",\n",
    "    ax=ax2,\n",
    "    color=\"orange\",\n",
    "    label=\"Energy\",\n",
    "    legend=False,\n",
    ")\n",
    "ax.figure.legend()\n",
    "ax.set_title(\n",
    "    f\"Accuracy and Energy, Final Trade-off Score: {trainer.pareto_tradeoff:.2f}\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Optimizing the trade-off\n",
    "\n",
    "Now, you will explore how different parameters affect the accuracy and energy consumption of the SNN. This part is open-ended, here are some ideas:\n",
    "\n",
    "- Experiment with network architectures (number of layers, number of neurons, etc.)\n",
    "- Implement a temporal loss (time-to-first-spike), using SnnTorch. Be careful to change the `calculate_accuracy` method in `training.py`\n",
    "- Implement a double exponential neuron model, using SnnTorch (snn.neurons.Synaptic)\n",
    "- Regularize spiking activity. \n",
    "- Implement weight masks to reduce the number of synapses\n",
    "- Use SnnTorch to make the time-constants heterogeneous and / or learnable, and maybe use less neurons\n",
    "\n",
    "Ideally, after experimenting with these parameters, you should start to see a ruff trade-off between accuracy and energy ! Can we see some kind of Pareto front appearing ? \n",
    "\n",
    "The group that will get the best trade-off score will win the competition ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
