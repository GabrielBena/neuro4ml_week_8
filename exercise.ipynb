{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ú® Neuro4ML Week 8: Neuromorphic Computing ‚ú®\n",
    "\n",
    "## üß† Introduction \n",
    "\n",
    "Neuromorphic engineering is a field that aims to design and build artificial neural systems that mimic the architecture and principles of biological neural networks. Unlike traditional von Neumann computing architectures, neuromorphic chips:\n",
    "\n",
    "1. üîÑ Process information in a parallel, event-driven manner\n",
    "2. üíæ Integrate memory and computation\n",
    "3. ‚ö° Operate with extremely low power consumption\n",
    "\n",
    "### ü§î Why Trade-off Power and Accuracy?\n",
    "\n",
    "Traditional deep learning models running on GPUs or CPUs consume significant power (often hundreds of watts). In contrast, the human brain processes complex information while consuming only ~20 watts. Neuromorphic chips aim to bridge this efficiency gap by:\n",
    "\n",
    "- üìä Using spike-based computation\n",
    "- üéØ Implementing local learning rules\n",
    "- ‚ö° Exploiting sparse, event-driven processing\n",
    "\n",
    "However, these benefits often come with reduced accuracy compared to traditional deep learning approaches. Understanding and optimizing this trade-off is crucial for deploying neural networks in power-constrained environments like mobile devices or IoT sensors.\n",
    "\n",
    "## üìù Exercise Overview\n",
    "\n",
    "In this exercise, you will:\n",
    "1. üîß Implement a simple neuromorphic chip simulator\n",
    "2. üèÉ‚Äç‚ôÇÔ∏è Train SNNs with different architectures\n",
    "3. üìä Analyze the power-accuracy trade-off\n",
    "4. üîç Explore how different parameters affect this trade-off\n",
    "\n",
    "This will also serve as a solid introduction on how to effectively train SNNs using modern packages such as SNNTorch! \n",
    "\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GabrielBena/neuro4ml_week_8/blob/main/exercise.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install snntorch\n",
    "    !git clone https://github.com/GabrielBena/neuro4ml_week_8.git\n",
    "    import sys\n",
    "    sys.path.append('neuro4ml_week_8')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For automatic reloading of external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from chip import NeuromorphicChip\n",
    "from models import SNNModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Exercise 1.1: Mapping Implementation\n",
    "\n",
    "To complete this first question you need to implement the functions necessary to map your network on the chip.\n",
    "\n",
    "- üìç Go to `models.py` and implement the `n_neurons` and `n_synapses` properties.\n",
    "- üìç Go to `chip.py` and implement the `calculate_memory_usage` and `map` methods.\n",
    "- ‚ñ∂Ô∏è Run the following cell to check your implementation\n",
    "\n",
    "This is what you should see:\n",
    "\n",
    "![Expected simulation results](sim_results.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip = NeuromorphicChip()\n",
    "\n",
    "dims = (128, 100, 10)\n",
    "n_timesteps = 100\n",
    "seed = 42\n",
    "snn = SNNModel(n_in=dims[0], n_hidden=dims[1], n_out=dims[-1], beta=0.95, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simulation Results:\n",
      "Energy consumption: 0.29 ¬µJ\n",
      "Memory usage: 57.34 KB\n",
      "Total neuron updates: 110\n",
      "Total synapse events: 553716\n",
      "Average spike rate: 0.219\n",
      "Total spikes: 4327.0\n"
     ]
    }
   ],
   "source": [
    "# Generate random input (seed is fixed to 42 for reproducibility)\n",
    "torch.manual_seed(seed)\n",
    "input_data = torch.randn(1, n_timesteps, dims[0]) * 10  # 100 timesteps\n",
    "\n",
    "# Map the network on the chip\n",
    "chip.map(snn)\n",
    "# Run the network\n",
    "output, results = chip.run(input_data=input_data)\n",
    "\n",
    "print(\"\\nSimulation Results:\")\n",
    "print(f\"Energy consumption: {results['total_energy_nJ']/1000:.2f} ¬µJ\")\n",
    "print(f\"Memory usage: {results['memory_usage_bytes']/1024:.2f} KB\")\n",
    "print(f\"Total neuron updates: {results['neuron_updates']}\")\n",
    "print(f\"Total synapse events: {results['synapse_events']}\")\n",
    "print(f\"Average spike rate: {results['spike_rate']:.3f}\")\n",
    "print(f\"Total spikes: {results['total_spikes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö´ Exercise 1.2: Failed Mappings\n",
    "\n",
    "Now let's explore what happens when we try to map networks that exceed the chip's constraints:\n",
    "\n",
    "### üî¨ Experiments:\n",
    "1. üß† First, we'll try mapping a network with too many neurons\n",
    "2. üîó Then, we'll attempt to map one with too many synapses \n",
    "3. üí° Finally, we'll see how sparse connectivity can help fit larger networks\n",
    "\n",
    "Let's run these experiments and observe the error messages we get! Each case will demonstrate different limitations of neuromorphic hardware:\n",
    "The first two cases should return a `MemoryError` if your code is correct. The third case should run without errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many neurons: 1034 (max: 1024)\n"
     ]
    }
   ],
   "source": [
    "chip = NeuromorphicChip()\n",
    "\n",
    "# Case 1 : Too many neurons\n",
    "dims = (128, 1024, 10)\n",
    "seed = 42\n",
    "snn = SNNModel(n_in=dims[0], n_hidden=dims[1], n_out=dims[-1], beta=0.95, seed=seed)\n",
    "# Map the network on the chip\n",
    "try : \n",
    "    chip.map(snn)\n",
    "except MemoryError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many synapses: 70656 (max: 65536)\n"
     ]
    }
   ],
   "source": [
    "chip = NeuromorphicChip()\n",
    "\n",
    "# Case 2 : Too many synapses\n",
    "dims = (128, 512, 10)\n",
    "seed = 42\n",
    "snn = SNNModel(n_in=dims[0], n_hidden=dims[1], n_out=dims[-1], beta=0.95, seed=seed)\n",
    "# Map the network on the chip\n",
    "try : \n",
    "    chip.map(snn)\n",
    "except MemoryError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped! Memory usage: 154.16 KB, Number of neurons: 522, Number of synapses: 35289\n"
     ]
    }
   ],
   "source": [
    "# Case 3 : Sparse connectivity\n",
    "dims = (128, 512, 10)\n",
    "seed = 42\n",
    "snn = SNNModel(n_in=dims[0], n_hidden=dims[1], n_out=dims[-1], beta=0.95, seed=seed)\n",
    "for l in snn.layers:\n",
    "    if hasattr(l, \"weight\"):\n",
    "        l.weight.data = torch.rand(l.weight.data.shape) < 0.5 # 50% of the weights are non-zero\n",
    "\n",
    "# Map the network on the chip\n",
    "try : \n",
    "    chip.map(snn)\n",
    "    print(f\"Mapped! Memory usage: {chip.calculate_memory_usage(snn)/1024:.2f} KB, Number of neurons: {snn.n_neurons}, Number of synapses: {snn.n_synapses}\")\n",
    "except MemoryError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 2: Proper Training\n",
    "\n",
    "In this exercise you will train a SNN on the Randman dataset.\n",
    "\n",
    "- üìù Go to `training.py` and complete the `SNNTrainer` class, in particular the `calculate_accuracy` method\n",
    "- ‚ñ∂Ô∏è Run the following cell to train your network\n",
    "- üìä Take a look at the training and testing metrics, especially the accuracy and energy consumption\n",
    "- üîÑ Start experimenting with different architectures and parameters to see how they affect the accuracy and energy consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import get_dataloaders, SNNTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_loader, test_loader, dataset = get_dataloaders(\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = next(iter(train_loader))\n",
    "print(data.shape, labels.shape)  # batch_size x timesteps x n_in. 1st and 2nd dims are swapped when passed to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn_config = {\n",
    "    \"n_hidden\": 128,\n",
    "    \"beta\": 0.95,\n",
    "    \"seed\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "snn = SNNModel(\n",
    "    n_hidden=snn_config[\"n_hidden\"],\n",
    "    beta=snn_config[\"beta\"],\n",
    "    seed=snn_config[\"seed\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = SNNTrainer(snn, learning_rate=1e-3, lr_gamma=0.9, config=snn_config)\n",
    "# Train the model\n",
    "trainer.train(train_loader, test_loader, n_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà Plot the results\n",
    "- üìä We can plot the accuracy and energy consumption as a function of the epoch\n",
    "- üìà We see that the accuracy is improving but the energy consumption is also increasing\n",
    "- ‚öñÔ∏è This is a trade-off that we need to be aware of when training SNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.pd_results\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(\n",
    "    data=results, x=\"epoch\", y=\"accuracy\", ax=ax, label=\"Accuracy\", legend=False\n",
    ")\n",
    "ax2 = ax.twinx()\n",
    "sns.lineplot(\n",
    "    data=results,\n",
    "    x=\"epoch\",\n",
    "    y=\"total_energy_nJ\",\n",
    "    ax=ax2,\n",
    "    color=\"orange\",\n",
    "    label=\"Energy\",\n",
    "    legend=False,\n",
    ")\n",
    "ax.figure.legend()\n",
    "ax.set_title(\n",
    "    f\"Accuracy and Energy, Final Trade-off Score: {trainer.pareto_tradeoff:.2f}\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Exercise 3: Optimizing the trade-off\n",
    "\n",
    "Now, you will explore how different parameters affect the accuracy and energy consumption of the SNN. This part is open-ended, here are some ideas:\n",
    "\n",
    "- üèóÔ∏è Experiment with network architectures (number of layers, number of neurons, etc.)\n",
    "- ‚è±Ô∏è Implement a temporal loss (time-to-first-spike), using SnnTorch. Be careful to change the `calculate_accuracy` method in `training.py`\n",
    "- üîÑ Implement a double exponential neuron model, using SnnTorch (snn.neurons.Synaptic)\n",
    "- üìä Regularize spiking activity \n",
    "- üéØ Implement weight masks to reduce the number of synapses\n",
    "- ‚ö° Use SnnTorch to make the time-constants heterogeneous and/or learnable, and maybe use less neurons\n",
    "\n",
    "Ideally, after experimenting with these parameters, you should start to see a rough trade-off between accuracy and energy! Can we see some kind of Pareto front appearing? \n",
    "\n",
    "### üèÜ *The group that will get the best trade-off score will win the competition!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
